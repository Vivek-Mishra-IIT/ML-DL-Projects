{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM GRU BRNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA3OGCTp4Dzh",
        "colab_type": "text"
      },
      "source": [
        "**LSTM GRU BRNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYo1nAoezzBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zMrcHqAnkXq",
        "colab_type": "text"
      },
      "source": [
        "We use Keras to load the imdb dataset, limiting it to the 10000 most common words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9uy6Huz9E_h",
        "colab_type": "code",
        "outputId": "66601136-698d-49e4-c5bb-4d6d38eefdaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "from tensorflow.keras.datasets import imdb \n",
        "\n",
        "num_words = 10000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "print(\"Number of exemples train set: %d\" % len(X_train))\n",
        "print(\"Number of exemples test set: %d\" % len(X_test))\n",
        "print(X_train[0])\n",
        "print(y_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "Number of exemples train set: 25000\n",
            "Number of exemples test set: 25000\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2bx_KRHn9ZD",
        "colab_type": "text"
      },
      "source": [
        "The reviews within the corpus of text obviously have different length, we use the pad_sequences function of keras to limit sequences to 500 elements (in our case limit sentences to 500 words). If a sequence has less than 500 examples, zeros will be added at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r65kNDrNte1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlen = 500\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen = maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen = maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2W3MVRboFCr",
        "colab_type": "text"
      },
      "source": [
        "**LSTM model**\n",
        "\n",
        "The first layer will embedding creating 100 embedding vectors for each of the 10,000 words in our dictionary.\n",
        "\n",
        "The second layer is the recurring Long-short term memory layer.\n",
        "\n",
        "The third layer will calculate the network ouput, being a binary classification problem (positive/negative review) the activation function will be the sigmoid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSwxIV__oBdl",
        "colab_type": "code",
        "outputId": "eb665a47-95f0-4fb5-e348-0a371a832183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 100))\n",
        "model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(LSTM(32, dropout=0.5, recurrent_dropout=0.2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "start_at = time()\n",
        "model.summary()\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 100)         1000000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 32)          17024     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,025,377\n",
            "Trainable params: 1,025,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "40/40 [==============================] - 132s 3s/step - loss: 0.6148 - accuracy: 0.6805 - val_loss: 0.4388 - val_accuracy: 0.8216\n",
            "Epoch 2/5\n",
            "40/40 [==============================] - 131s 3s/step - loss: 0.3966 - accuracy: 0.8383 - val_loss: 0.4050 - val_accuracy: 0.8240\n",
            "Epoch 3/5\n",
            "40/40 [==============================] - 131s 3s/step - loss: 0.3213 - accuracy: 0.8738 - val_loss: 0.3448 - val_accuracy: 0.8564\n",
            "Epoch 4/5\n",
            "40/40 [==============================] - 129s 3s/step - loss: 0.2854 - accuracy: 0.8910 - val_loss: 0.3266 - val_accuracy: 0.8628\n",
            "Epoch 5/5\n",
            "40/40 [==============================] - 132s 3s/step - loss: 0.2595 - accuracy: 0.9056 - val_loss: 0.3305 - val_accuracy: 0.8566\n",
            "782/782 [==============================] - 160s 204ms/step - loss: 0.3354 - accuracy: 0.8558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3354117274284363, 0.8557999730110168]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMCvKVVAte6h",
        "colab_type": "text"
      },
      "source": [
        "**GRU**\n",
        "\n",
        "Unlike LSTMs, Gated Recurrent Units (GRUs) require fewer tensor calculations and therefore usually lead to similar results in less time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqNTQmBxodUK",
        "colab_type": "code",
        "outputId": "83c93b81-6232-404b-a6d9-29aa69a5052a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, GRU, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 100))\n",
        "model.add(GRU(32, dropout=0.5, recurrent_dropout=0.2, return_sequences=True))\n",
        "model.add(GRU(32, dropout=0.5, recurrent_dropout=0.2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "start_at = time()\n",
        "model.summary()\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 100)         1000000   \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, None, 32)          12864     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 32)                6336      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,019,233\n",
            "Trainable params: 1,019,233\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "40/40 [==============================] - 124s 3s/step - loss: 0.6462 - accuracy: 0.6199 - val_loss: 0.5461 - val_accuracy: 0.7252\n",
            "Epoch 2/5\n",
            "40/40 [==============================] - 122s 3s/step - loss: 0.4574 - accuracy: 0.7907 - val_loss: 1.2087 - val_accuracy: 0.5986\n",
            "Epoch 3/5\n",
            "40/40 [==============================] - 120s 3s/step - loss: 0.3634 - accuracy: 0.8484 - val_loss: 0.3483 - val_accuracy: 0.8496\n",
            "Epoch 4/5\n",
            "40/40 [==============================] - 118s 3s/step - loss: 0.3157 - accuracy: 0.8708 - val_loss: 0.3499 - val_accuracy: 0.8606\n",
            "Epoch 5/5\n",
            "40/40 [==============================] - 117s 3s/step - loss: 0.2796 - accuracy: 0.8878 - val_loss: 0.3815 - val_accuracy: 0.8408\n",
            "782/782 [==============================] - 157s 200ms/step - loss: 0.3795 - accuracy: 0.8410\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37949562072753906, 0.8409600257873535]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL7D5sdm2hHl",
        "colab_type": "text"
      },
      "source": [
        "**BRNN** (Bidirectional Recurrent Neural Networks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhOPgwwUpIhk",
        "colab_type": "code",
        "outputId": "b4df82db-57c2-4f4d-a921-c342a12d489b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        }
      },
      "source": [
        "from tensorflow.keras.layers import  Embedding, Dense, LSTM, Activation, Bidirectional, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 100))\n",
        "model.add(Bidirectional(LSTM(32, dropout=0.5, recurrent_dropout=0.2, return_sequences = True)))\n",
        "model.add(Bidirectional(LSTM(32, dropout=0.5, recurrent_dropout=0.2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "start_at = time()\n",
        "model.summary()\n",
        "model.fit(X_train, y_train, batch_size=512, validation_split=0.2, epochs=5)\n",
        "\n",
        "model.evaluate(X_test, y_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 100)         1000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, None, 64)          34048     \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 64)                24832     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 1,058,945\n",
            "Trainable params: 1,058,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "40/40 [==============================] - 261s 7s/step - loss: 0.6077 - accuracy: 0.6581 - val_loss: 0.5167 - val_accuracy: 0.7312\n",
            "Epoch 2/5\n",
            "40/40 [==============================] - 259s 6s/step - loss: 0.4033 - accuracy: 0.8382 - val_loss: 0.5176 - val_accuracy: 0.7990\n",
            "Epoch 3/5\n",
            "40/40 [==============================] - 260s 6s/step - loss: 0.3330 - accuracy: 0.8701 - val_loss: 0.3361 - val_accuracy: 0.8702\n",
            "Epoch 4/5\n",
            "40/40 [==============================] - 258s 6s/step - loss: 0.2917 - accuracy: 0.8896 - val_loss: 0.3571 - val_accuracy: 0.8424\n",
            "Epoch 5/5\n",
            "40/40 [==============================] - 258s 6s/step - loss: 0.2583 - accuracy: 0.9107 - val_loss: 0.3163 - val_accuracy: 0.8750\n",
            "782/782 [==============================] - 317s 406ms/step - loss: 0.3270 - accuracy: 0.8671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.32699453830718994, 0.8671200275421143]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYcMFCUKN5ic",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Comparison of the networks\n",
        "\n",
        "* From my experience, GRUs train faster and perform better than LSTMs on less training data if you are doing language modeling (not sure about other tasks).\n",
        "* GRUs are simpler and thus easier to modify, for example adding new gates in case of additional input to the network. It’s just less code in general.\n",
        "* LSTMs should, in theory, remember longer sequences than GRUs and outperform them in tasks requiring modeling long-distance relations.\n",
        "* The GRUs also have less parameter complexity than LSTM which can be seen from the model summaries above.\n",
        "* The simple RNNs only have simple recurrent operations without any gates to control the flow of information among the cells.\n",
        "* BRNN are doing almost double recurrence so taking more time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T54R0FdV5JvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}