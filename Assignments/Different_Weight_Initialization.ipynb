{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq9fiwaot5z7",
        "colab_type": "text"
      },
      "source": [
        "#                      Backpropagation Learning Programming Assignment \n",
        "\n",
        "\n",
        "*   Write a program for the backpropagation algorithm.\n",
        "*   No. of layers, No. of neurons and type of output function, in each layer can\n",
        "be inputs.\n",
        "*   Demonstrating training and testing for task of your choice. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4k2MWWqK2bK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPbJ8Ch0E6aL",
        "colab_type": "text"
      },
      "source": [
        "### Loading MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-WmP4pqE3Gx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, Y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "Y = np.array(list(map(int, Y)))\n",
        "\n",
        "one_idx = Y==1\n",
        "zero_idx = Y==0\n",
        "idx = np.logical_or(one_idx, zero_idx)\n",
        "# print(len(one_idx), len(zero_idx), len(idx))\n",
        "X = X[idx]\n",
        "Y = Y[idx]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvdnxDV6ODNq",
        "colab_type": "code",
        "outputId": "2be72e1f-ccea-4d29-abeb-71ed6db2bbd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.33, random_state=42)\n",
        "# np.reshape(y_train(9902,1))\n",
        "y_train = y_train.reshape((-1,1))\n",
        "y_test = y_test.reshape((-1,1))\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9902, 784) (9902, 1)\n",
            "(4878, 784) (4878, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dLmR9PjF3Rn",
        "colab_type": "code",
        "outputId": "ee7ca71c-5840-4f49-c14a-c873ea328cf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train[0].reshape((28,28)))\n",
        "print(x_train)\n",
        "print(y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[1]\n",
            " [0]\n",
            " [1]\n",
            " ...\n",
            " [0]\n",
            " [1]\n",
            " [1]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndiAJwg4kQdiBJAg7kARhB5Ig7EAS/w8U0hSxDFDT9gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHHk9_TOtk7N",
        "colab_type": "code",
        "outputId": "ccd8d390-a3f0-46a4-a81a-6e6372a60b0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "X_train = np.transpose(x_train/255)\n",
        "print(\"\\nOutput is based on (No. of of 1's)%2\")\n",
        "Y_train = np.transpose(y_train)\n",
        "X_test = np.transpose(x_test/255)\n",
        "Y_test = np.transpose(y_test)\n",
        "print (\"\\nTraining Input ->\") \n",
        "print(X_train.shape)\n",
        "print (\"\\nTraining Output ->\") \n",
        "print(Y_train.shape)\n",
        "print (\"\\nTesting Input ->\") \n",
        "print(X_test.shape)\n",
        "print (\"\\nTesting Output ->\") \n",
        "print(Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output is based on (No. of of 1's)%2\n",
            "\n",
            "Training Input ->\n",
            "(784, 9902)\n",
            "\n",
            "Training Output ->\n",
            "(1, 9902)\n",
            "\n",
            "Testing Input ->\n",
            "(784, 4878)\n",
            "\n",
            "Testing Output ->\n",
            "(1, 4878)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRHCmo3Wu_59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implements the sigmoid activation in numpy\n",
        "    \n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of sigmoid(z), same shape as Z\n",
        "    cache -- returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "    \n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the RELU function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter, of the same shape as Z\n",
        "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFyzFnyy-PyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: initialize_parameters_deep\n",
        "\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        \n",
        "        # Random Intialization between 0-1:\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) \n",
        "        #He initialization:\n",
        "        #parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*np.sqrt(2) / np.sqrt(layer_dims[l-1]) \n",
        "        #Xavier initialization:\n",
        "        #parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
        "\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwJCBsim-T4z",
        "colab_type": "code",
        "outputId": "32fb9d09-768b-41ca-b70f-87af0ee89bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "parameters = initialize_parameters_deep([3,2,1])\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = [[ 1.62434536 -0.61175641 -0.52817175]\n",
            " [-1.07296862  0.86540763 -2.3015387 ]]\n",
            "b1 = [[0.]\n",
            " [0.]]\n",
            "W2 = [[ 1.74481176 -0.7612069 ]]\n",
            "b2 = [[0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mrA2yIYmwor",
        "colab_type": "text"
      },
      "source": [
        "## Forward propagation \n",
        "\n",
        "### Linear Forward\n",
        "The linear forward module (vectorized over all the examples) computes the following equations:\n",
        "\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
        "\n",
        "where $A^{[0]} = X$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wa7oM7x-kkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = W.dot(A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlPMK8VGans2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTIBBecFnXv9",
        "colab_type": "text"
      },
      "source": [
        "### L-Layer Model \n",
        "\n",
        "For even more convenience when implementing the $L$-layer Neural Net, we will need a function that replicates the previous one (`linear_activation_forward` with RELU) $L-1$ times, then follows that with one `linear_activation_forward` with SIGMOID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhHn3LV1ayr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
        "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIu5BmuSnnze",
        "colab_type": "text"
      },
      "source": [
        "## Cost function\n",
        "\n",
        "Now We will implement forward and backward propagation. We need to compute the cost, because we want to check if our model is actually learning.\n",
        "\n",
        "Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjH-8zd6a-_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy8AxKr7oKW7",
        "colab_type": "text"
      },
      "source": [
        "## Backward propagation \n",
        "\n",
        "Just like with forward propagation, you will implement helper functions for backpropagation. Remember that back propagation is used to calculate the gradient of the loss function with respect to the parameters. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TbjKl4NusIB",
        "colab_type": "text"
      },
      "source": [
        "[Chain Rule](https://drive.google.com/open?id=1k4PyaKRlMNFnE2GHmpXhzU1NQbbBTaJU)\n",
        "\n",
        "<caption><center> <br> *The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.*  </center></caption>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7SOLn6ixpIU",
        "colab_type": "text"
      },
      "source": [
        "### Linear backward\n",
        "\n",
        "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
        "\n",
        "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. We want to get $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
        "\n",
        "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGBK5USBbB6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
        "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoSlkoVQyKPs",
        "colab_type": "text"
      },
      "source": [
        "### Linear-Activation backward\n",
        "\n",
        "Next, We create a function that merges the two helper functions: **`linear_backward`** and the backward step for the activation **`linear_activation_backward`**. \n",
        "\n",
        "To  implement `linear_activation_backward`, provided two backward functions:\n",
        "- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit. You can call it as follows:\n",
        "\n",
        "```python\n",
        "dZ = sigmoid_backward(dA, activation_cache)\n",
        "```\n",
        "\n",
        "- **`relu_backward`**: Implements the backward propagation for RELU unit. You can call it as follows:\n",
        "\n",
        "```python\n",
        "dZ = relu_backward(dA, activation_cache)\n",
        "```\n",
        "\n",
        "If $g(.)$ is the activation function, \n",
        "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybvm5-L1oHJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fQK-gsHyuQB",
        "colab_type": "text"
      },
      "source": [
        "### L-Model Backward \n",
        "\n",
        "Now We implement the backward function for the whole network. We will iterate through all the hidden layers backward, starting from layer $L$. On each step, you will use the cached values for layer $l$ to backpropagate through layer $l$.\n",
        "\n",
        "#### Initializing backpropagation:\n",
        "\n",
        "To backpropagate through this network, we know that the output is, \n",
        "$A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
        "To do so, use this formula (derived using calculus which you don't need in-depth knowledge of):\n",
        "```python\n",
        "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
        "```\n",
        "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra4mfbEQyd3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
        "    \n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"sigmoid\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxMT90F-0ZHX",
        "colab_type": "text"
      },
      "source": [
        "### Update Parameters\n",
        "\n",
        "In this section We will update the parameters of the model, using gradient descent: \n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
        "\n",
        "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYbubndR0QWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "        \n",
        "    return parameters\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhzk9Jt606cR",
        "colab_type": "text"
      },
      "source": [
        "## 5 - L-layer Neural Network\n",
        "\n",
        "\n",
        "```python\n",
        "def initialize_parameters_deep(layers_dims):\n",
        "    ...\n",
        "    return parameters \n",
        "def L_model_forward(X, parameters):\n",
        "    ...\n",
        "    return AL, caches\n",
        "def compute_cost(AL, Y):\n",
        "    ...\n",
        "    return cost\n",
        "def L_model_backward(AL, Y, caches):\n",
        "    ...\n",
        "    return grads\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    ...\n",
        "    return parameters\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H5XgybO5nd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m_train = 7\n",
        "num_px = 3\n",
        "m_test = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDY82HSr0fOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### CONSTANTS ###\n",
        "layers_dims = [784,20, 7, 5,1] #  L-layer model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tptd44Io0_9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: L_layer_model\n",
        "\n",
        "def L_layer_model(X_train, Y_train, layers_dims, learning_rate = 0.05, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. (≈ 1 line of code)\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    \n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "        \n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        \n",
        "        AL, caches = L_model_forward(X_train, parameters)\n",
        "        \n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, Y_train)\n",
        "    \n",
        "        # Backward propagation.\n",
        "        grads = L_model_backward(AL, Y_train, caches)\n",
        "        \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        # print(parameters)\n",
        "        # print(\"\\n\")\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 50 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 50 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-n6oihqXKfv",
        "colab_type": "code",
        "outputId": "2c0a198c-f9d9-4d1f-c746-7503b629f995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        }
      },
      "source": [
        "parameters = L_layer_model((X_train),(Y_train), layers_dims, num_iterations = 1000, print_cost = True) # Xavier\n",
        "        #Xavier initialization:\n",
        "        #parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.692425\n",
            "Cost after iteration 50: 0.686714\n",
            "Cost after iteration 100: 0.677010\n",
            "Cost after iteration 150: 0.655827\n",
            "Cost after iteration 200: 0.609800\n",
            "Cost after iteration 250: 0.526424\n",
            "Cost after iteration 300: 0.431368\n",
            "Cost after iteration 350: 0.360715\n",
            "Cost after iteration 400: 0.310368\n",
            "Cost after iteration 450: 0.271352\n",
            "Cost after iteration 500: 0.238207\n",
            "Cost after iteration 550: 0.208913\n",
            "Cost after iteration 600: 0.182844\n",
            "Cost after iteration 650: 0.159716\n",
            "Cost after iteration 700: 0.139383\n",
            "Cost after iteration 750: 0.121865\n",
            "Cost after iteration 800: 0.106888\n",
            "Cost after iteration 850: 0.094225\n",
            "Cost after iteration 900: 0.083518\n",
            "Cost after iteration 950: 0.074430\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEWCAYAAAAAZd6JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMXrzLuxX1es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, y, parameters):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    parameters -- parameters of the trained model\n",
        "    \n",
        "    Returns:\n",
        "    p -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    p = np.zeros((1,m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_model_forward(X, parameters)\n",
        "\n",
        "    \n",
        "    # convert probas to 0/1 predictions\n",
        "    for i in range(0, probas.shape[1]):\n",
        "        if probas[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "        else:\n",
        "            p[0,i] = 0\n",
        "    \n",
        "    #print results\n",
        "    #print (\"predictions: \" + str(p))\n",
        "    #print (\"true labels: \" + str(y))\n",
        "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
        "        \n",
        "    return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R5dEaPxX_Ud",
        "colab_type": "code",
        "outputId": "e2e3b2a7-1b34-41a4-ea09-5fa533704889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_train = predict(X_train, Y_train, parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9867703494243584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP-eJSmuYF1F",
        "colab_type": "code",
        "outputId": "05705199-a75a-4e9b-8e57-82e70b8e67fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_train = predict(X_test, Y_test, parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9874948749487495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDEzu2rH1Eke",
        "colab_type": "code",
        "outputId": "9dc197ab-fb4c-418d-ea6a-cef9fe75c310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        }
      },
      "source": [
        "parameters = L_layer_model((X_train),(Y_train), layers_dims, num_iterations = 1000, print_cost = True) \n",
        "        #He initialization:\n",
        "        #parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*np.sqrt(2) / np.sqrt(layer_dims[l-1]) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.690452\n",
            "Cost after iteration 50: 0.664922\n",
            "Cost after iteration 100: 0.598762\n",
            "Cost after iteration 150: 0.486354\n",
            "Cost after iteration 200: 0.379369\n",
            "Cost after iteration 250: 0.300066\n",
            "Cost after iteration 300: 0.246120\n",
            "Cost after iteration 350: 0.202193\n",
            "Cost after iteration 400: 0.164646\n",
            "Cost after iteration 450: 0.133433\n",
            "Cost after iteration 500: 0.108611\n",
            "Cost after iteration 550: 0.089343\n",
            "Cost after iteration 600: 0.074517\n",
            "Cost after iteration 650: 0.063154\n",
            "Cost after iteration 700: 0.054416\n",
            "Cost after iteration 750: 0.047563\n",
            "Cost after iteration 800: 0.042088\n",
            "Cost after iteration 850: 0.037734\n",
            "Cost after iteration 900: 0.034217\n",
            "Cost after iteration 950: 0.031337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEWCAYAAAAAZd6JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n75dBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag80uBpz1QiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, y, parameters):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    parameters -- parameters of the trained model\n",
        "    \n",
        "    Returns:\n",
        "    p -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    p = np.zeros((1,m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_model_forward(X, parameters)\n",
        "\n",
        "    \n",
        "    # convert probas to 0/1 predictions\n",
        "    for i in range(0, probas.shape[1]):\n",
        "        if probas[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "        else:\n",
        "            p[0,i] = 0\n",
        "    \n",
        "    #print results\n",
        "    #print (\"predictions: \" + str(p))\n",
        "    #print (\"true labels: \" + str(y))\n",
        "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
        "        \n",
        "    return p\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0VNVZOp1BLZ",
        "colab_type": "code",
        "outputId": "1dfde411-3e08-4f64-92d7-7f6b24c07824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_train = predict(X_train, Y_train, parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9933346798626537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kspXv5HM1JzR",
        "colab_type": "code",
        "outputId": "93a1d65f-1d41-4175-8d23-6f5bd65641ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_train = predict(X_test, Y_test, parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9938499384993851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGpKvZHsvtqH",
        "colab_type": "code",
        "outputId": "3230040e-a11d-451b-8c66-e77d7b21cab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "source": [
        "parameters = L_layer_model((X_train),(Y_train), layers_dims, num_iterations = 1000, print_cost = True)        \n",
        "        # Random Intialization between 0-1:\n",
        "        #parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 3.303103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:75: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 50: nan\n",
            "Cost after iteration 100: nan\n",
            "Cost after iteration 150: nan\n",
            "Cost after iteration 200: nan\n",
            "Cost after iteration 250: nan\n",
            "Cost after iteration 300: nan\n",
            "Cost after iteration 350: nan\n",
            "Cost after iteration 400: nan\n",
            "Cost after iteration 450: nan\n",
            "Cost after iteration 500: nan\n",
            "Cost after iteration 550: nan\n",
            "Cost after iteration 600: nan\n",
            "Cost after iteration 650: nan\n",
            "Cost after iteration 700: nan\n",
            "Cost after iteration 750: nan\n",
            "Cost after iteration 800: nan\n",
            "Cost after iteration 850: nan\n",
            "Cost after iteration 900: nan\n",
            "Cost after iteration 950: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAEWCAYAAAAw6c+oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkFUn_Ujd7CA",
        "colab_type": "code",
        "outputId": "8c74b205-d919-4382-e305-0f92b40689d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_train = predict(X_train, Y_train, parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.461320945263583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ3MeQOod7mG",
        "colab_type": "code",
        "outputId": "18cec7e4-76e9-46dd-96c4-7480bc8f0e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred_train = predict(X_test, Y_test, parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.47867978679786793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrCEb1xxfPwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}